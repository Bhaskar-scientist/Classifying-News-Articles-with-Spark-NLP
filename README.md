# Classifying-News-Articles-with-Spark-NLP
ğŸ“° Classifying News Articles with Spark NLP
Built a scalable text classification model using Spark NLP, PySpark ML, and John Snow Labs' pre-trained models. The project processes and classifies 2,225 BBC News articles into five categories: business, entertainment, politics, sport, and tech.

ğŸ“Œ Project Highlights
ğŸ“„ Dataset: 2,225 labeled BBC news articles

ğŸ§  Goal: Accurately classify each article into one of the five categories

âš™ï¸ Tech Stack: PySpark, Spark NLP, Spark MLlib, Pandas, John Snow Labs NLP models

ğŸ›  Pipeline Includes:

Data ingestion and cleaning

Text preprocessing (tokenization, lemmatization, stopwords removal)

Feature extraction (TF-IDF, CountVectorizer)

Model training using Spark ML algorithms

Evaluation using accuracy, F1-score, and precision/recall

â˜ï¸ Scalable GCP-Based Architecture (Suggested for Production)
While the current implementation runs locally, the project is designed to be production-ready and can be deployed using the following Google Cloud Platform architecture:

pgsql
Copy
Edit
              +-----------------------------+
              |   Google Cloud Storage (GCS)|
              |   (Raw Data Input)          |
              +-------------+---------------+
                            |
                            v
           +-------------------------------+
           |  Dataproc Cluster (PySpark Job)|
           |  - Spark NLP Pipeline          |
           |  - Model Training & Inference  |
           +-------------------------------+
                            |
                            v
              +-----------------------------+
              |  BigQuery (Model Outputs)   |
              +-----------------------------+
                            |
                            v
            +--------------------------------+
            |  Streamlit / Flask Web App     |
            |  (For real-time predictions)   |
            +--------------------------------+
âš™ï¸ Future enhancement: Export the trained model and deploy it as a Cloud Function or REST API with real-time input and output handling.

ğŸ§ª Tools & Libraries Used
Apache Spark & PySpark â€“ Distributed processing

Spark NLP (John Snow Labs) â€“ Efficient NLP on Spark

Spark MLlib â€“ Machine learning on large datasets

Jupyter Notebook â€“ Interactive development

ğŸ“ Repository Structure
vbnet
Copy
Edit
ğŸ“¦ Classifying-News-Articles-with-Spark-NLP
 â”£ ğŸ““ Spark NLP & ML for Text Classification.ipynb
 â”£ ğŸ“œ README.md
ğŸ“ˆ Results
The trained model demonstrated robust accuracy across all five news categories. PySpark's distributed architecture and Spark NLP's optimized components ensured efficient handling of structured and unstructured text data at scale.

ğŸš€ Getting Started
1. Clone the Repo
bash
Copy
Edit
git clone https://github.com/Bhaskar-scientist/Classifying-News-Articles-with-Spark-NLP.git
cd Classifying-News-Articles-with-Spark-NLP
2. Install Dependencies
bash
Copy
Edit
pip install pyspark spark-nlp pandas
3. Launch Jupyter Notebook
bash
Copy
Edit
jupyter notebook
Open and run:
ğŸ““ Spark NLP & ML for Text Classification.ipynb

ğŸ”® Future Improvements
Deploy classifier via Streamlit or Flask UI

Integrate the pipeline with Google Cloud Platform

Serve predictions via REST API (e.g., FastAPI + Cloud Run)

Visualize key insights: class distribution, confusion matrix, keyword clouds

Experiment with transformer-based models (e.g., BERT) using Spark NLP

ğŸ‘¨â€ğŸ’» Author
Bhaskar Reddy
Data Scientist | NLP & ML Enthusiast
ğŸ”— LinkedIn | ğŸ›  Portfolio (Coming soon)

â­ï¸ If you found this helpful...
Give the repository a star â­ï¸, fork it, or share with fellow developers working on NLP, PySpark, or cloud-native data engineering!
